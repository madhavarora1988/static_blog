Having tried a few of the [Qwen 3 models](https://huggingface.co/models?search=qwen3) now my favorite is a bit of a surprise to me: I'm really enjoying [Qwen3-8B](https://huggingface.co/mlx-community/Qwen3-8B-4bit).

I've been running prompts through the MLX 4bit quantized version, [mlx-community/Qwen3-8B-4bit](https://huggingface.co/mlx-community/Qwen3-8B-4bit). I'm using [llm-mlx](https://github.com/jmorganca/llm-mlx) like this:

```
llm install llm-mlx
llm mlx download-model mlx-community/Qwen3-8B-4bit
```

This pulls 4.3GB of data and saves it to ~/.cache/huggingface/hub/models--mlx-community--Qwen3-8B-4bit.

I assigned it a default alias:

```
llm aliases set q3 mlx-community/Qwen3-8B-4bit
```

I also added a default option for that model - this saves me from adding `-o unlimited 1` to every prompt which disables the default output token limit:

```
llm models options set q3 unlimited 1
```

And now I can run prompts: 